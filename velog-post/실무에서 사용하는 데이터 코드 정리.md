<p>! pip install seaborn # seaborn 설치
! pip install scikit-learn 설치 </p>
<p>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
%matplotlib inline  # 주피터 노트북에서 그래프나 시각화를 내장된 형태로 바로 표시
import seaborn as sns
import sklearn as sk</p>
<p>pd.read_csv('파일이름.csv')
df.head() # 디폴트로 상위 , 하위 5줄 보여줌
df.tail()
df.info() # 데이터프레임 정보 : 컬럼정보 , 데이터 수 , 데이터 타입 , null 개수 
df.index  # 데이터프레임 인덱스 정보 : 인덱스 범위와 규칙적인 인덱스 구조
df.columns 
df.values
df.describe() # 데이터프레임 통계치</p>
<p>df.isnull().sum() # null 존재 및 합계
df['컬럼']  # 컬럼 데이터 확인 # 전체 열을 출력
df['컬럼'].unique() # 중복된 값 제거 
df['컬럼'].value_count<strong>s()</strong> # 컬럼 데이터 별 건수 
df['voc_trt_perd_itg_cd'].value_counts(<strong>normalize = True</strong>) # 비율 구해줌
df.drop(columns = '컬럼', inplace=True) 
= df.drop('컬럼' , axis=1 , inplace=True)
=df1 = df.drop('customerID' , axis =1)
col = ['컬럼1'  , '컬럼2' , '컬럼3'  , '컬럼4' , '컬럼5']
df.drop(col, axis=1, inplace=True) # 여러개 col 삭제 
df.dropna(inplace=True)</p>
<p>(df['cust_clas_itg_cd'] == '<em>').sum()
= df['cust_clas_itg_cd'].value_counts().get('</em>') # '<em>' 개수 확인 
df.replace('</em>', np.nan) # 값 변경 
df.dtypes # 컬럼 타입확인</p>
<p>TotalCharge의 컬럼 타입을 확인하는 코드를 작성하세요.</p>
<ul>
<li>df1['TotalCharges'].dtype
' ' 값을 0으로 변환하고 컬럼 타입을 float로 변경하세요.</li>
<li></li>
</ul>
<p>df = df.astype({'컬럼': 'int'}) 컬럼 값들 int형으로 변경 
df['컬럼'] = pd.to_numeric(df['컬럼'], errors='coerce') pd.to_numeric : 문자열을 숫자형으로 변환하는 함수 , errors='coerce': 숫자가 아닌 값들 NaN으로 변경</p>
<p>모든 열에서 결측치가 있는 행 제거
df4.dropna(inplace=True)</p>
<p>df['컬럼'].fillna(df['컬럼'].median(), inplace = True) # NaN 값 중앙값으로 </p>
<p>결측치 비율 계산
missing_ratio = df4.isnull().sum(normalize=True)</p>
<p>결측치가 40% 이상인 컬럼 찾기
high_missing_cols = missing_ratio[missing_ratio &gt;= 0.4]</p>
<p>cat_cols = df5.select_dtypes('object') # select_dtypes() # 특정 데이터 타입을 선택하는 함수</p>
<p>f4에서 컬럼의 데이터 타입이 object인 컬럼들을 원-핫 인코딩하세요.</p>
<ul>
<li>컬럼의 데이터 타입이 object인 컬럼들을 object_cols 변수에 저장하세요.</li>
<li>object_cols 변수의 컬럼들을 원-핫 인코딩하세요.</li>
<li>전처리된 데이터를 df5에 저장하세요.
object_cols = df4.select_dtypes('object').columns.values
df5 = pd.get_dummies(data=df4, columns=object_cols</li>
</ul>
<hr />
<p>df1['TotalCharges'].dtype
df1['TotalCharges'].replace([' '], ['0'], inplace=True)
df1['TotalCharges'] = df1['TotalCharges'].astype(float)
df2=df1.copy()</p>
<hr />
<p>df2['Churn'].value_counts()
df3 = df2.copy()
df3['Churn'] = df2['Churn'].replace(['Yes', 'No'], [1, 0])</p>
<hr />
<p>print(df3.isnull().sum())
df4 = df3.copy()
df4.drop('DeviceProtection', axis=1, inplace=True)
df4.dropna(inplace=True)
df4.info()</p>
<hr />
<p>df4['SeniorCitizen'].value_counts().plot(kind='bar')
df4.drop('SeniorCitizen', axis=1, inplace=True)
df4.info()</p>
<hr />
<p>import seaborn as sns
import matplotlib.pyplot as plt
sns.histplot(data=df4, x='tenure')
plt.show()</p>
<p>sns.kdeplot(data=df4, x='tenure', hue='Churn')
plt.show()
print('O')</p>
<p>sns.heatmap(df4[['tenure','MonthlyCharges','TotalCharges']].corr(), annot=True)
print(0.83)</p>
<hr />
<p>from sklearn.model_selection import train_test_split</p>
<p>X = df5.drop('Churn', axis=1).values
y = df5['Churn'].values</p>
<p>X_train, X_valid, y_train, y_valid = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    stratify=y,
                                                    random_state=42)</p>
<hr />
<p>from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)</p>
<p>le = LabelEncoder()
df5['cust_clas_itg_cd'] = le.fit_transform(df5['cust_clas_itg_cd'])</p>
<blockquote>
<p>✔️ <strong>df4 = df3</strong>  , *<em>df5 = df3.copy() 차이점 *</em></p>
</blockquote>
<ul>
<li>df4 = df3          # 참조 ( 얇은 복사 : df3도 함께 변경됨 ) </li>
<li>df5 = df3.copy()   # 복사 ( 깊은 복사 :  df5는 변경되지 않음) </li>
</ul>
<ul>
<li>LogisticRegression 모델 정의하고 학습시키세요. </li>
<li>KNN으로 모델을 정의하고 학습시키세요. (n_neighbors=5)</li>
<li>Decision Tree로 모델을 정의하고 학습시키세요. (max_depth=10, random_state=42)</li>
<li>RandomForest로 모델을 정의하고 학습시키세요. (n_estimators=3, random_state=42)</li>
<li>XGBoost로 모델을 정의하고 학습시키세요. (n_estimators=3, random_state=42)  </li>
<li>Light GBM으로 모델을 정의하고 학습시키세요. (n_estimators=3, random_state=42)  </li>
</ul>
<hr />
<p>from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()
lg.fit(X_train, y_train)</p>
<hr />
<p>from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)</p>
<hr />
<p>from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(max_depth=10, random_state=42)
dt.fit(X_train, y_train)</p>
<hr />
<p>from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=3, random_state=42)
rfc.fit(X_train, y_train)</p>
<hr />
<p>!pip install xgboost
from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators=3, random_state=42)<br />xgb.fit(X_train, y_train)</p>
<hr />
<p>!pip install lightgbm
from lightgbm import LGBMClassifier
lgbm = LGBMClassifier(n_estimators=3, random_state=42)<br />lgbm.fit(X_train, y_train)</p>
<hr />
<p>from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report</p>
<p>y_pred = lgbm.predict(X_valid)
cm = confusion_matrix(y_valid, y_pred)
sns.heatmap(cm, annot=True)</p>
<p>print(classification_report(y_valid, y_pred, zero_division=1))</p>
<hr />
<p>import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical</p>
<p>tf.random.set_seed(1)</p>
<hr />
<ul>
<li>Tensoflow framework를 사용하여 딥러닝 모델을 만드세요.</li>
<li>히든레이어(hidden layer) 2개이상으로 모델을 구성하세요.</li>
<li>dropout 비율 0.2로 Dropout 레이어 1개를 추가해 주세요.</li>
<li>하이퍼파라미터 epochs: 30, batch_size: 16으로 설정해주세요.</li>
<li>각 에포크마다 loss와 metrics 평가하기 위한 데이터로 X_valid, y_valid 사용하세요.</li>
<li>학습정보는 history 변수에 저장해주세요</li>
</ul>
<h1 id="여기에-답안코드를-작성하세요">여기에 답안코드를 작성하세요.</h1>
<p>model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))</p>
<p>model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) </p>
<p>es = EarlyStopping(monitor='val_loss', patience=5) </p>
<p>checkpoint_path = 'best_model.keras'
mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True)</p>
<p>history = model.fit(X_train, y_train, epochs=30, batch_size=16,
                   validation_data = (X_valid, y_valid),
                    callbacks=[es, mc]
                    )</p>
<hr />
<ul>
<li>1개의 그래프에 학습 accuracy와 검증 accuracy 2가지를 모두 표시하세요.</li>
<li>위 2가지 각각의 범례를 'acc', 'val_acc'로 표시하세요.</li>
<li>그래프의 타이틀은 'Accuracy'로 표시하세요.</li>
<li>X축에는 'Epochs'라고 표시하고 Y축에는 'Acc'라고 표시하세요.</li>
</ul>
<p>plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(['acc', 'val_acc'])
plt.show()</p>